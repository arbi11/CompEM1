{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C_tf2_CNN_BC_Lives_DyNA_3_Dec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbi11/CompEM1/blob/master/A2C_tf2_CNN_BC_Lives_DyNA_3_Dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwHlIc9U7pby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "M_NO                = 1\n",
        "MACHINE             = 'Acer'\n",
        "#PATH = \"D:\\\\Documents\\\\Codes\\\\RL-Acer\\\\RL\\\\PPO\"\n",
        "PATH                = 'C:\\\\Users\\\\chetanm\\\\Desktop\\\\Deep Learning\\\\A2C_tf2_CNN_stack'\n",
        "\n",
        "MODEL_PATH          = 'Ampere_Data_Eval_DQL_1N3_A2C_NL_STACK_5_33_lives'\n",
        "TRAIN_PATH          = 'Ampere_Data_Train_DQL_1N3_A2C_NL'\n",
        "max_steps           = 75\n",
        "env_dim             = [18, 35]\n",
        "action_dim          = 8\n",
        "state_size          = [4, 15, 28]\n",
        "max_iron            = 190\n",
        "\n",
        "lr                  = 0.0007\n",
        "batch_size          = 1024\n",
        "miniBatchSize       = 512\n",
        "penalty             = -2\n",
        "\n",
        "gamma               = 0.95                             # Discounting rate\n",
        "playTime            = 2\n",
        "\n",
        "warmUpSteps         = 500 #200\n",
        "experienceLength    = 2500\n",
        "hidden_size         = [512, 256]\n",
        "variance            = 0.4\n",
        "num_episodes        = 500\n",
        "\n",
        "max_eps             = 1.0\n",
        "min_eps             = 0.01\n",
        "decay_rate          = 0.001\n",
        " \n",
        "tau                 = 0.001\n",
        "entropy             = 0.001\n",
        "value_multiplier    = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3LltzWn7qKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import femm\n",
        "import imageio\n",
        "import warnings\n",
        "from  skimage import img_as_float, img_as_uint\n",
        "#import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "import constants as con\n",
        "from shutil import copyfile\n",
        "import pickle\n",
        "\n",
        "FEMM_PATH = con.PATH + '\\\\'\n",
        "#FEMM_PATH = 'C:\\\\Users\\\\Arbi\\\\Desktop\\\\Arbi\\\\'\n",
        "MODEL_PATH = con.MODEL_PATH + '\\\\'\n",
        "TRAIN_PATH = con.TRAIN_PATH + '\\\\'\n",
        "\n",
        "class WormFemmEnv4:\n",
        "\n",
        "    def __init__(self, env_dim = [21, 35], startR=3 , startC= 4, max_steps= con.max_steps):\n",
        "        \n",
        "        self.env_dim = env_dim\n",
        "        self.step_count = 0\n",
        "        self.posR = startR\n",
        "        self.posC = startC\n",
        "        self.done = 0.0\n",
        "        self.repeat = 0\n",
        "        self.issue = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.past_rewards = []\n",
        "        self.past_states = []\n",
        "        self.past_actions = []\n",
        "        self.state_hist = []\n",
        "        self.past_BnStates = []\n",
        "        self.net_force = 0.0\n",
        "        self.R = 0.0\n",
        "        self.count = 0\n",
        "        self.buffer = 3\n",
        "        self.B_state = np.zeros([15,])\n",
        "        \n",
        "        self.coil_start = 2\n",
        "        self.coil_width = 3\n",
        "        self.coil_length = 9\n",
        "        \n",
        "        self.coil_current_choice = np.array([1, 1.5])\n",
        "        self.coil_turns_choice = np.array([500])\n",
        "        \n",
        "        self.arm_start = 27\n",
        "        self.arm_width = 6\n",
        "        self.arm_length = 15\n",
        "        \n",
        "        self.cont_start_col = self.coil_start + self.coil_width\n",
        "        self.window_width = self.arm_start - (self.coil_start + self.coil_width)\n",
        "        self.window_length = np.max([self.arm_length, 15])\n",
        "        self.frame_idx = 0\n",
        "        self.worm_step_size = 3\n",
        "        \n",
        "        self.lives = 0.75\n",
        "        self.material_left = (con.max_iron - 9)/con.max_iron\n",
        "        self.steps_left = (con.max_steps - 1)/con.max_steps\n",
        "        \n",
        "    \n",
        "    def create_geo(self):\n",
        "        \n",
        "        femm.openfemm(1);\n",
        "        femm.opendocument(FEMM_PATH + 'actuator.fem');\n",
        "\n",
        "        femm.mi_getmaterial('Air')\n",
        "#        femm.mi_addmaterial('LinearIron', 2100, 2100, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
        "        femm.mi_getmaterial('Cold rolled low carbon strip steel')            \n",
        "        femm.mi_getmaterial('Copper')\n",
        "        femm.mi_addcircprop('icoil', 1, 1);\n",
        "        \n",
        "        # femm.mi_setblockprop('Air', 0, 10, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        for i in range(17):\n",
        "            for j in range(35):\n",
        "                \n",
        "                femm.mi_selectlabel(i + 0.5, j + 0.5);\n",
        "                \n",
        "        femm.mi_setblockprop('Air', 0, 0.5, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        coil2_start = 8 + 8 - (self.coil_start + self.coil_width)\n",
        "        femm.mi_clearselected();\n",
        "        \n",
        "        femm.mi_selectrectangle(0 + 0.1, self.coil_start + 0.1, self.coil_length - 0.1, self.coil_start + self.coil_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()\n",
        "        femm.mi_drawrectangle(0, self.coil_start, self.coil_length, self.coil_start + self.coil_width)\n",
        "        femm.mi_addblocklabel((0 + self.coil_length)/ 2, (2*self.coil_start + self.coil_width)/ 2)\n",
        "        \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectrectangle(0 + 0.1, coil2_start + 0.1, self.coil_length - 0.1, coil2_start + self.coil_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()        \n",
        "        femm.mi_drawrectangle(0, coil2_start, self.coil_length, coil2_start + self.coil_width)\n",
        "        femm.mi_addblocklabel((0 + self.coil_length)/ 2, (2*coil2_start + self.coil_width)/ 2)\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectlabel((0 + self.coil_length)/ 2, (2*coil2_start + self.coil_width)/ 2);\n",
        "        femm.mi_setblockprop('Copper', 0, 0, 'icoil', 0, 1, 500); \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectlabel((0 + self.coil_length)/ 2, (2*self.coil_start + self.coil_width)/ 2)\n",
        "        femm.mi_setblockprop('Copper', 0, 0, 'icoil', 0, 1, -500); \n",
        "        \n",
        "        #        femm.mi_setblockprop('Copper', 0, 0.25, 'icoil', 0, 1, 500);        \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectrectangle(0 + 0.1, self.arm_start + 0.1, self.arm_length - 0.1, self.arm_start + self.arm_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()        \n",
        "        femm.mi_drawrectangle(0, self.arm_start, self.arm_length, self.arm_start + self.arm_width)\n",
        "        femm.mi_addblocklabel((0 + self.arm_length)/ 2, (2*self.arm_start + self.arm_width)/ 2)\n",
        "        femm.mi_selectlabel((0 + self.arm_length)/ 2, (2*self.arm_start + self.arm_width)/ 2);\n",
        "        femm.mi_setblockprop('Cold rolled low carbon strip steel', 0, 0.5, '<None>', 0, 5, 0);\n",
        "        femm.mi_clearselected();\n",
        "                \n",
        "        femm.mi_drawrectangle(0, 100, 100, -100)\n",
        "        femm.mi_addblocklabel(50, 0);\n",
        "        femm.mi_selectlabel(50, 0);\n",
        "        femm.mi_setblockprop('Air', 50, 0, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_addboundprop('neumann', 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0)\n",
        "        femm.mi_selectrectangle(-0.5, 100.5, 0.2, -100.5, 4)\n",
        "        femm.mi_setsegmentprop('neumann', 0, 0, 0, 5)\n",
        "        femm.mi_clearselected();       \n",
        "        \n",
        "        femm.mi_selectsegment(95, -95)\n",
        "        femm.mi_selectsegment(55, 95)\n",
        "        femm.mi_selectsegment(55, -95)\n",
        "        femm.mi_addboundprop('dirichlet', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
        "        femm.mi_setsegmentprop('dirichlet', 0, 0, 0, 0)\n",
        "        femm.mi_clearselected();       \n",
        "\n",
        "        femm.mi_zoomnatural();\n",
        "        femm.mi_saveas(FEMM_PATH + 'actuator2.fem');  \n",
        "        femm.closefemm()\n",
        "        \n",
        "        #####################################################################################\n",
        "        #                                   VARIABLES                                       #       \n",
        "        #####################################################################################\n",
        "        \n",
        "        # step_count           :       step# in an episode.\n",
        "        # done          :       episode over or not.\n",
        "        # snap          :       store material dist as png (mat2gray).\n",
        "        # act_py        :       action (material dist) sent to matlab.\n",
        "        # state_abs     :       action with discrete values(0/1) & flattened.\n",
        "        # changes       :       changes made between (n-1) & 'n'th actions.\n",
        "        # act_mat       :       action as matlab double.\n",
        "        # count         :       No. of iron (1's) in the action.\n",
        "        # frame_idx     :       episode #.\n",
        "        \n",
        "        #####################################################################################\n",
        "        # mat_res       :       output from matlab:                                         #\n",
        "        #                               nargout= 2 (means 2 variables from matlab)          #\n",
        "        # field_xy      :                   var1 = Bx, By (concatenated) dim: (2, 21, 15)   #\n",
        "        # Fx            :                   var2 = Force magnitude dim: 1(scalar)           #\n",
        "        #####################################################################################\n",
        "        \n",
        "        # reward        :       Reward for the agent (Fx(n) - Fx(n-1)).\n",
        "        # state3        :       environment state for the agent. dim: (3, 21, 15).\n",
        "        \n",
        "        #####################################################################################\n",
        "        \n",
        "    def move(self, mat_state):\n",
        "        \n",
        "        if (self.worm_step_size == 3):\n",
        "            mat_state[self.posR -1 : self.posR + 2, self.posC -1 : self.posC + 2] = 4    \n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "        elif (self.worm_step_size == 1 and self.past_actions[-1]<4):\n",
        "            mat_state[self.posR -1 : self.posR + 2, self.posC -1 : self.posC + 2] = 4    \n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "        else:\n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "\n",
        "        return(mat_state)\n",
        "\n",
        "    def reward(self, mat_state):\n",
        "        \n",
        "        #print('entered REWARD process with a shape of:', mat_state.shape)\n",
        "\n",
        "        femm.openfemm(1);\n",
        "        femm.opendocument(FEMM_PATH + 'actuator2.fem');\n",
        "        #femm.mi_addmaterial('LinearIron', 2100, 2100, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
        "\n",
        "        indices = np.nonzero(mat_state[self.buffer:-1, :] - self.state_hist[-1])\n",
        "        \n",
        "        len_ind = np.size(indices[0])\n",
        "        for i in range(len_ind):\n",
        "            #print(indices[0][i], ',', indices[1][i])\n",
        "            femm.mi_selectlabel(indices[0][i] + 0.4, indices[1][i] + 0.4);\n",
        "            femm.mi_deleteselectedlabels()\n",
        "            femm.mi_addblocklabel(indices[0][i] + 0.4, indices[1][i] + 0.4)\n",
        "            femm.mi_clearselected();\n",
        "\n",
        "        for i in range(len_ind):\n",
        "            femm.mi_selectlabel(indices[0][i] + 0.4, indices[1][i] + 0.4);\n",
        "            femm.mi_setblockprop('Cold rolled low carbon strip steel', 0, 0, '<None>', 0, 3, 0);\n",
        "            \n",
        "        femm.mi_clearselected();\n",
        "        femm.mi_zoomnatural()\n",
        "        femm.mi_saveas('actuator2.fem')\n",
        "        femm.mi_analyse(0)\n",
        "        femm.mi_loadsolution()\n",
        "                \n",
        "        Bx, By = femm.mo_getb(0.2, 27.12)\n",
        "        B = np.sqrt(Bx ** 2 + By ** 2)\n",
        "        femm.mo_groupselectblock(5)\n",
        "        self.net_force = femm.mo_blockintegral(19) *(-1)\n",
        "                  \n",
        "        Bxy_state = np.zeros([self.env_dim[0]-self.buffer, self.env_dim[1], 2])\n",
        "        for r in range(Bxy_state.shape[0]):\n",
        "            for c in range(self.env_dim[1]):\n",
        "                Bxy_state[r, c, :] = femm.mo_getb(r+0.5, c+0.5)\n",
        "            \n",
        "        self.B_state = np.sum(np.square(Bxy_state), axis=-1)                \n",
        "        femm.closefemm()        \n",
        "        reward = B*1000\n",
        "                \n",
        "        return(reward)            \n",
        "        \n",
        "    def check_actual_movement(self, clean_state):\n",
        "        clean_state [clean_state ==5] = 4\n",
        "        clean_past_state = np.copy(self.past_states[-1])\n",
        "        clean_past_state [clean_past_state == 5] = 4\n",
        "        \n",
        "        if np.array_equal(clean_state, clean_past_state):\n",
        "            #print('same as before')\n",
        "            return True            \n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def clean_up(self, mat_state):\n",
        "        \n",
        "        mat_state[self.buffer: self.buffer + self.coil_length, \n",
        "                  self.coil_start: self.coil_start + self.coil_width] = -self.coil_current\n",
        "        coil2_start = 8 + 8 - (self.coil_start + self.coil_width)\n",
        "        mat_state[:, 0:self.coil_start] = -3\n",
        "        mat_state[self.buffer: self.buffer + self.coil_length, \n",
        "                  coil2_start : coil2_start + self.coil_width] = -self.coil_current\n",
        "                  \n",
        "        mat_state[self.buffer + self.coil_length:, \n",
        "                  self.coil_start: self.coil_start + self.coil_width] = -3                  \n",
        "        mat_state[0:self.buffer, :] = -3\n",
        "        mat_state[:, self.arm_start- 1] = -3\n",
        "        mat_state[self.window_length + self.buffer:, :] = -3\n",
        "        mat_state[self.buffer: self.buffer + self.arm_length, \n",
        "                  self.arm_start : self.arm_start + self.arm_width] = -4\n",
        "                  \n",
        "        mat_state[self.buffer + self.coil_length:, self.coil_start+1] = np.copy(self.lives)\n",
        "        mat_state[self.buffer + self.coil_length:, self.coil_start] = np.copy(self.material_left)\n",
        "        mat_state[self.buffer + self.coil_length:, self.coil_start+2] = np.copy(self.steps_left)\n",
        "\n",
        "                          \n",
        "        mat_state[mat_state == 5] = 4\n",
        "        mat_state[self.posR, self.posC] = 5\n",
        "\n",
        "        return mat_state\n",
        "    \n",
        "    ##### ACTIONS #######\n",
        "    # 0     : soft RIGHT#\n",
        "    # 1     : soft LEFT #\n",
        "    # 2     : soft UP   #\n",
        "    # 3     : soft DOWN #\n",
        "    ##-----------------##\n",
        "    # 4     : RIGHT     #\n",
        "    # 5     : LEFT      #\n",
        "    # 6     : UP        #\n",
        "    # 7     : DOWN      #\n",
        "    #####################\n",
        "    \n",
        "    def step(self, action):\n",
        "        \n",
        "        state = np.copy(self.past_states[-1])\n",
        "        self.step_count +=1\n",
        "        self.R = 0.0\n",
        "        self.count = 0\n",
        "\n",
        "        if action == 0:\n",
        "            self.worm_step_size = 1\n",
        "            self.posC += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.issue = 2\n",
        "                self.posC -= self.worm_step_size\n",
        "                        \n",
        "        elif action == 1:\n",
        "            self.worm_step_size = 1                               \n",
        "            self.posC -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posC += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 2:\n",
        "            self.worm_step_size = 1                               \n",
        "            self.posR -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posR += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 3:\n",
        "            self.worm_step_size = 1    \n",
        "            self.posR += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 1\n",
        "            else:\n",
        "                self.posR -= self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 4:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3  \n",
        "                \n",
        "            self.posC += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.issue = 2\n",
        "                self.posC -= self.worm_step_size\n",
        "                        \n",
        "        elif action == 5:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3                    \n",
        "            \n",
        "            self.posC -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posC += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 6:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3\n",
        "                \n",
        "            self.posR -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posR += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 7:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3\n",
        "            self.posR += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 4:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 1\n",
        "            else:\n",
        "                self.posR -= self.worm_step_size\n",
        "                self.issue = 2\n",
        "                \n",
        "        if self.issue == 0:    \n",
        "            #print('Going for reward with a shape of:', state2.shape)\n",
        "            new_state = self.clean_up(state2)\n",
        "            F = self.reward(new_state)\n",
        "            self.R = F - np.copy(self.past_forces[-1])\n",
        "#            R = F\n",
        "            self.past_rewards.append(self.R)\n",
        "            self.past_forces.append(np.copy(F))\n",
        "           \n",
        "        elif self.issue == 1:\n",
        "                                 \n",
        "            new_state = self.clean_up(state2)\n",
        "            if not self.check_actual_movement(np.copy(new_state)):\n",
        "                \n",
        "                #print('Not the same geo. Going for reward')\n",
        "                F = self.reward(new_state)\n",
        "                self.R = F - np.copy(self.past_forces[-1])           \n",
        "                self.past_rewards.append(self.R)\n",
        "                self.past_forces.append(np.copy(F))\n",
        "            else:\n",
        "                self.R = 0     \n",
        "                self.past_rewards.append(self.R)                      \n",
        "           \n",
        "        elif self.issue == 2:\n",
        "            \n",
        "            new_state = state\n",
        "            pos_live = 0\n",
        "            \n",
        "            if state[self.posR, self.posC] == 2:\n",
        "                self.R = con.penalty\n",
        "                # coil\n",
        "            elif state[self.posR, self.posC] == 3:\n",
        "                self.R = con.penalty - 1\n",
        "                # outside air\n",
        "            else:\n",
        "                self.R = con.penalty\n",
        "                        \n",
        "            if len(self.past_actions) > 4:            \n",
        "                self.repeat_count = all(elem == self.past_actions[-4] for elem in self.past_actions[-3:])\n",
        "            else:\n",
        "                self.repeat_count = 0\n",
        "                \n",
        "            if len(self.past_rewards) > 4:                \n",
        "                self.neg_reward = all(elem < 0 for elem in self.past_rewards[-4:])\n",
        "            else:\n",
        "                self.neg_reward = 0\n",
        "            \n",
        "            if self.posC>20 or self.posR >9:\n",
        "                pos_live = 1 \n",
        "            else:\n",
        "                pos_live = 0\n",
        "                \n",
        "            if self.repeat_count > 0 and self.neg_reward > 0 and pos_live>0:\n",
        "                self.posC = self.coil_start + self.coil_width + self.worm_step_size -2\n",
        "                self.posR = self.worm_step_size + 1\n",
        "                self.lives -= 0.25\n",
        "                new_state = self.clean_up(state)\n",
        "                self.R = con.penalty * con.penalty_mul\n",
        "                \n",
        "            self.past_rewards.append(self.R)\n",
        "            \n",
        "        unique, counts = np.unique(new_state, return_counts=True)\n",
        "        index_4 = np.where (unique == 4)\n",
        "        index_5 = np.where (unique == 5)\n",
        "        #print(index_5)\n",
        "        #print(index_1)\n",
        "        try:\n",
        "            self.count = np.asscalar(counts[index_4[0]] + 1)            \n",
        "            \n",
        "        except:\n",
        "            self.count = 0        \n",
        "            \n",
        "        self.material_left = (con.max_iron - np.copy(self.count))/con.max_iron\n",
        "        self.steps_left = (con.max_steps - np.copy(self.step_count))/con.max_steps\n",
        "        new_state = self.clean_up(new_state)\n",
        "        \n",
        "        if self.count > con.max_iron or self.step_count == self.max_steps or self.lives<0.1:\n",
        "            self.done = 1.0            \n",
        "        \n",
        "        if new_state[self.posR, self.posC] != 5:\n",
        "            raise('Problem in code. Worm outside the geometry')\n",
        "            \n",
        "        elif np.asscalar(counts[index_5[0]]) > 1:\n",
        "            raise('Problem in code. More than one worm heads <<MEDUSA>>')\n",
        "        \n",
        "        self.past_states.append(np.copy(new_state))\n",
        "        self.past_actions.append(np.copy(action))\n",
        "#        , \"Changes:\", changes-22\n",
        "        \n",
        "        new_state2 = new_state[self.buffer: self.buffer+15, self.coil_start:27]\n",
        "        new_state3 = np.dstack((new_state2, np.copy(self.B_state[:15, self.coil_start:27])))                \n",
        "\n",
        "        self.past_BnStates.append(np.copy(new_state3))\n",
        "        \n",
        "#        fd = open(eps_dir + str(frame_idx) + '.csv','a')\n",
        "#        writer=csv.writer(fd,delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
        "#        writer.writerow([self.step_count, R, count])\n",
        "#        fd.close()\n",
        "#        new_state = np.expand_dims(new_state, axis= -1)    \n",
        "#        state_index = np.argmax(new_state[1:16, 5:26])\n",
        "\n",
        "        new_state4 = np.copy(new_state3.transpose((2, 0, 1)))\n",
        "        \n",
        "        return(new_state4, self.R, self.done, {})\n",
        "#            , self.issue\n",
        "#############################################################################\n",
        "#                             Reset Environment                             #       \n",
        "#############################################################################\n",
        "        \n",
        "    def reset(self, geo_dynamic= False):\n",
        "        \n",
        "    #    state_dim = [21, 31]  # size of geometry\n",
        "\n",
        "        self.worm_step_size = 3  \n",
        "        self.lives = 0.75\n",
        "        self.material_left = (con.max_iron - 9)/con.max_iron\n",
        "        self.steps_left = (con.max_steps - 1)/con.max_steps\n",
        "\n",
        "        self.done = 0.0\n",
        "        \n",
        "        self.max_steps = con.max_steps\n",
        "        self.past_rewards = []\n",
        "        self.past_states = []\n",
        "        self.past_actions = []\n",
        "        self.state_hist = []\n",
        "        self.past_BnStates = []\n",
        "        self.past_forces = []\n",
        "        self.net_force = 0.0\n",
        "        self.R = 0.0\n",
        "        self.count = 0                \n",
        "        self.step_count = 0\n",
        "        self.B_state = np.zeros([15,])\n",
        "        \n",
        "        if geo_dynamic == True:\n",
        "            self.coil_start = np.random.randint(1, 6)        \n",
        "            self.arm_start = np.random.randint(25, 31)\n",
        "            self.coil_width = np.random.randint(3, 8- self.coil_start + 1)\n",
        "            self.coil2_start = np.random.randint(self.coil_start + self.coil_width + 4, self.coil_start + self.coil_width + 8)  \n",
        "            self.arm_width = np. random.randint(3, 35 - self.arm_start)\n",
        "            self.arm_length = np.random.randint(13, 17)\n",
        "            self.window_length = np.random.randint(self.arm_length, 17)\n",
        "            self.coil_length = np.random.randint(6, self.window_length -5)\n",
        "            self.window_width = self.arm_start - self.coil_start - self.coil_width - 1\n",
        "            self.coil_turns = 500\n",
        "            self.coil_current = 1\n",
        "\n",
        "        else:               \n",
        "            self.coil_start = 2\n",
        "            self.arm_start = 27\n",
        "            self.coil_width = 3\n",
        "            self.coil2_start = 8 + 8 - (self.coil_start + self.coil_width)\n",
        "            self.arm_width = 6\n",
        "            self.arm_length = 15\n",
        "            self.window_length = 15\n",
        "            self.coil_length = 9\n",
        "            self.window_width = 21\n",
        "            self.coil_turns = np.random.choice(self.coil_turns_choice)\n",
        "            self.coil_current = np.random.choice(self.coil_current_choice)                  \n",
        "\n",
        "        self.create_geo()\n",
        "                \n",
        "        \n",
        "        self.buffer = 3\n",
        "        ob = np.ones(self.env_dim)*(-3)\n",
        "        \n",
        "        ob[self.buffer : self.buffer + self.window_length, \n",
        "           self.coil_start + self.coil_width : \n",
        "               self.coil_start + self.coil_width + self.window_width] = 0\n",
        "        \n",
        "        ob[self.buffer: self.buffer + self.coil_length, \n",
        "           self.coil_start: self.coil_start + self.coil_width] = -self.coil_current\n",
        "        \n",
        "        coil2_start = 8 + 8 - (self.coil_start + self.coil_width)\n",
        "        \n",
        "        ob[self.buffer: self.buffer + self.coil_length , \n",
        "           coil2_start : coil2_start + self.coil_width] = -self.coil_current\n",
        "        \n",
        "        ob[self.buffer: self.buffer + self.arm_length, \n",
        "           self.arm_start : self.arm_start + self.arm_width] = -4\n",
        "           \n",
        "        ob[self.buffer + self.coil_length:, self.coil_start+1] = np.copy(self.lives)\n",
        "        ob[self.buffer + self.coil_length:, self.coil_start] = np.copy(self.material_left)\n",
        "        ob[self.buffer + self.coil_length:, self.coil_start+2] = np.copy(self.steps_left)\n",
        "\n",
        "        #print('RESETing with a state[',self.posR,',',self.posC, '] = ', ob[self.posR, self.posC])\n",
        "        self.posC = self.posC = self.coil_start + self.coil_width + self.worm_step_size -2\n",
        "        self.posR = self.worm_step_size + 1\n",
        "                  \n",
        "        self.state_hist.append((np.copy(ob[self.buffer:-1, :])))\n",
        "        \n",
        "        ob = self.move(ob)    \n",
        "#        ob = self.clean_up(ob)\n",
        "        self.past_states.append(np.copy(ob))\n",
        "        self.past_actions.append(4)\n",
        "        F = self.reward(ob)\n",
        "        self.past_forces.append(np.copy(F))\n",
        "        self.past_rewards.append(np.copy(F))\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        imageio.imwrite(FEMM_PATH + 'first.png', img_as_uint(np.abs(ob)/np.max(ob)))\n",
        "        self.frame_idx += 1\n",
        "        \n",
        "        ob2 = ob[self.buffer: self.buffer + 15, self.coil_start:27]\n",
        "        ob3 = np.dstack((ob2, np.copy(self.B_state[:15, self.coil_start:27])))\n",
        "        \n",
        "        self.past_BnStates.append(np.copy(ob3))        \n",
        "        ob4 = np.copy(ob3.transpose((2, 0, 1)))\n",
        "\n",
        "        return(ob4)\n",
        "\n",
        "    def render(self, train= False):\n",
        "\n",
        "        if train == True:    \n",
        "            eps_dir = FEMM_PATH + TRAIN_PATH + '\\\\Eps_' + str(self.frame_idx) + '\\\\'\n",
        "        else:\n",
        "            eps_dir = FEMM_PATH + MODEL_PATH + 'Eps_' + str(self.frame_idx) + '\\\\'   \n",
        "        \n",
        "        if not (os.path.exists(FEMM_PATH + MODEL_PATH)):\n",
        "            os.mkdir(FEMM_PATH + MODEL_PATH)\n",
        "\n",
        "        if not (os.path.exists(eps_dir)):\n",
        "            os.mkdir(eps_dir)\n",
        "\n",
        "        toP = np.copy(self.past_states[-1])\n",
        "        toP = toP[self.buffer : self.buffer+15, 5:26]\n",
        "#        toP[toP == 1] = 5\n",
        "        toP = np.abs(toP)/np.max(np.abs(toP))\n",
        "        imageio.imwrite(eps_dir + str(self.frame_idx) + '_Step-' + \n",
        "                        str(self.step_count) + '_Issue-' + str(self.issue) + \n",
        "                        '_Amp-' + str(self.coil_current) + \n",
        "                        '_reward-' + str(self.R) + '_action-' + \n",
        "                        str(self.past_actions[-1]) + '_lives-' + \n",
        "                        str(self.lives)+ '_posR-'+ str(self.posR) + \n",
        "                        '_posC-'+ str(self.posC) + '.png', img_as_uint(toP))\n",
        "        \n",
        "def save_file(src, fileName):\n",
        "    dst = FEMM_PATH + MODEL_PATH + fileName + '.py'    \n",
        "    copyfile(src, dst)\n",
        "\n",
        "def get_file(path_file):\n",
        "    path_components = path_file.split('\\\\')\n",
        "    for component in path_components:\n",
        "        file_sre = re.search('(.*).py', component)\n",
        "#        file_name = file_sre.group(1)\n",
        "    return file_sre     \n",
        "\n",
        "def get_file_back_slash(path_file):\n",
        "    path_components = path_file.split('/')\n",
        "    for component in path_components:\n",
        "        file_sre = re.search('(.*).py', component)\n",
        "#        file_name = file_sre.group(1)\n",
        "    return file_sre     \n",
        "\n",
        "def create_txt_file(txt, name):\n",
        "    with open(FEMM_PATH + MODEL_PATH + name + \"_history.txt\", \"w\") as myfile:\n",
        "        myfile.write(txt)\n",
        "        \n",
        "def append_txt_file(txt, name):\n",
        "    with open(FEMM_PATH + MODEL_PATH + name + \"_history.txt\", \"a\") as myfile:\n",
        "            myfile.write(txt)\n",
        "      \n",
        "def save_obj(obj, name):    \n",
        "    with open(FEMM_PATH + MODEL_PATH + name + '.pkl', 'wb') as f:        \n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)            \n",
        "\n",
        "def load_obj(name ):    \n",
        "    with open('obj/' + name + '.pkl', 'rb') as f:        \n",
        "        return pickle.load(f)            \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogSpVAQb7tax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import gym\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "import tensorflow.keras.losses as kls\n",
        "import tensorflow.keras.optimizers as ko\n",
        "\n",
        "import constants as con\n",
        "from env_Worm_B_1N3_B_Channel_5_33 import WormFemmEnv2, append_txt_file, create_txt_file\n",
        "\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "class ProbabilityDistribution(tf.keras.Model):\n",
        "    def call(self, logits):\n",
        "        # sample a random categorical action from given logits\n",
        "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__('mlp_policy')\n",
        "        # no tf.get_variable(), just simple Keras API\n",
        "        self.conv1 = kl.Convolution2D(filters= 8, kernel_size = 3, padding= \"same\", activation='relu')\n",
        "        self.conv2 = kl.Convolution2D(filters= 4, kernel_size = 3, padding= \"same\", activation='relu')\n",
        "        self.flatten = kl.Flatten()\n",
        "        \n",
        "        self.hidden1 = kl.Dense(con.hidden_size[0], activation='relu')\n",
        "        self.hidden2 = kl.Dense(con.hidden_size[1], activation='relu')\n",
        "        \n",
        "        self.value = kl.Dense(1, name='value')\n",
        "        \n",
        "        # logits are unnormalized log probabilities\n",
        "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
        "        self.dist = ProbabilityDistribution()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs is a numpy array, convert to Tensor\n",
        "        x = tf.convert_to_tensor(inputs)\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        # separate hidden layers from the same input tensor\n",
        "        hidden_logs = self.hidden1(x)\n",
        "        hidden_vals = self.hidden2(x)\n",
        "        \n",
        "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
        "\n",
        "    def action_value(self, obs):\n",
        "        # executes call() under the hood\n",
        "        logits, value = self.predict(obs)\n",
        "        action = self.dist.predict(logits)\n",
        "        # a simpler option, will become clear later why we don't use it\n",
        "        # action = tf.random.categorical(logits, 1)\n",
        "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\n",
        "    \n",
        "class A2CAgent:\n",
        "    def __init__(self, model):\n",
        "        # hyperparameters for loss terms, gamma is the discount coefficient\n",
        "        self.params = {\n",
        "            'gamma'     : con.gamma,\n",
        "            'value'     : con.value_multiplier,\n",
        "            'entropy'   : con.entropy\n",
        "        }\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            optimizer=ko.RMSprop(lr=con.lr),\n",
        "            # define separate losses for policy logits and value estimate\n",
        "            loss=[self._logits_loss, self._value_loss]\n",
        "        )\n",
        "    \n",
        "    def train(self, env, batch_sz=con.max_steps*3, updates=500):\n",
        "        # storage helpers for a single batch of data\n",
        "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
        "        rewards, dones, values = np.empty((3, batch_sz))\n",
        "        observations = np.empty((batch_sz,) +  tuple(con.state_size))\n",
        "        # training loop: collect samples, send to optimizer, repeat updates times\n",
        "        ep_rews = [0.0]\n",
        "        next_obs = env.reset()\n",
        "        next_obs = np.concatenate((np.copy(next_obs), np.copy(next_obs)), axis=0)\n",
        "\n",
        "        for update in range(updates):\n",
        "            for step in range(batch_sz):\n",
        "                observations[step] = next_obs.copy()\n",
        "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
        "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
        "                \n",
        "                next_obs = np.concatenate((np.copy(next_obs), np.copy(observations[-1, :2, :, :])), axis= 0)\n",
        "#                env.render(train=True)\n",
        "                ep_rews[-1] += rewards[step]\n",
        "                if dones[step]:\n",
        "                    ep_rews.append(0.0)                    \n",
        "                    logging.info(\"Episode: %03d, Reward: %.2f, Count: %d, Net Force: %.3f\" % (env.frame_idx, ep_rews[-2], env.count, env.net_force))\n",
        "                    append_txt_file(\"\\n Episode: %03d, Reward: %.2f, Count: %d, Net Force: %.3f\" % (env.frame_idx, ep_rews[-2], env.count, env.net_force), file_name)\n",
        "                    next_obs = env.reset()\n",
        "                    next_obs = np.concatenate((np.copy(next_obs), np.copy(next_obs)), axis=0)\n",
        "\n",
        "            _, next_value = self.model.action_value(next_obs[None, :])\n",
        "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
        "            # a trick to input actions and advantages through same API\n",
        "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
        "            # performs a full training step on the collected batch\n",
        "            # note: no need to mess around with gradients, Keras API handles it\n",
        "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
        "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n",
        "            \n",
        "            if update % con.playTime == 0:\n",
        "                print('Playing after Epoch: {:d} and update {:d} ---> \\n'.format(env.frame_idx, update))\n",
        "                append_txt_file('\\n \\t Playing after Epoch: {:d} and update {:d} ---> \\n'.format(env.frame_idx, update), file_name)\n",
        "                rewards_sum, net_force, iron_c = agent.test(env)\n",
        "                \n",
        "                print(\"Total Episode Reward: {:.2f} iron_c: {} with net force {:.2f}\".format(rewards_sum, iron_c, net_force))\n",
        "                append_txt_file(\"\\n \\t Total Episode Reward: {:.2f} iron_c: {} with net force {:.2f}\".format(rewards_sum, iron_c, net_force), file_name)\n",
        "                \n",
        "        return ep_rews\n",
        "\n",
        "    def test(self, env, render=True):\n",
        "        obs, done, ep_reward = env.reset(), False, 0\n",
        "        obs = np.concatenate((np.copy(obs), np.copy(obs)), axis=0)\n",
        "\n",
        "        while not done:\n",
        "            action, _ = self.model.action_value(obs[None, :])\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            obs = np.concatenate((np.copy(next_obs), np.copy(obs[:2, :, :])), axis= 0)\n",
        "\n",
        "            ep_reward += reward\n",
        "            if render:\n",
        "                env.render(train= False)                \n",
        "        return ep_reward, env.net_force, env.count\n",
        "\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
        "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "        # returns are calculated as discounted sum of future rewards\n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
        "        returns = returns[:-1]\n",
        "        # advantages are returns - baseline, value estimates in our case\n",
        "        advantages = returns - values\n",
        "        return returns, advantages\n",
        "    \n",
        "    def _value_loss(self, returns, value):\n",
        "        # value loss is typically MSE between value estimates and returns\n",
        "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
        "\n",
        "    def _logits_loss(self, acts_and_advs, logits):\n",
        "        # a trick to input actions and advantages through same API\n",
        "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
        "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
        "        # from_logits argument ensures transformation into normalized probabilities\n",
        "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        # policy loss is defined by policy gradients, weighted by advantages\n",
        "        # note: we only calculate the loss on the actions we've actually taken\n",
        "        actions = tf.cast(actions, tf.int32)\n",
        "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
        "        # entropy loss can be calculated via CE over itself\n",
        "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
        "        # here signs are flipped because optimizer minimizes\n",
        "        return policy_loss - self.params['entropy']*entropy_loss\n",
        "        \n",
        "env = WormFemmEnv2()\n",
        "model = Model(num_actions=con.action_dim)\n",
        "#model.action_value(env.reset()[None, :])  \n",
        "\n",
        "#env = gym.make('CartPole-v0')\n",
        "#model = Model(num_actions=env.action_space.n)\n",
        "agent = A2CAgent(model)  \n",
        "\n",
        "file_name = \"A2C_lr_{:.4f}_gamma_{:.2f}_entropy_{:.4f}_value_{:.2f}\".format(con.lr, con.gamma, con.entropy, con.value_multiplier)\n",
        "text = 'A2C_Tf2_Stack_5_33'\n",
        "create_txt_file(text, file_name)\n",
        "\n",
        "#rewards_sum = agent.test(env)\n",
        "#print(\"Total Episode Reward: %d out of 200\" % agent.test(env))\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "rewards_history = agent.train(env)\n",
        "print(\"Finished training.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}