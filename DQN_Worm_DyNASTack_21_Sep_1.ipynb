{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DQN_Worm_Dy_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbi11/CompEM1/blob/master/DQN_Worm_DyNASTack_21_Sep_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CICchMeUcst",
        "colab_type": "code",
        "outputId": "b9c583b8-c331-4eab-88b8-97df06798416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\chetanm'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lHZ29sVUeIl",
        "colab_type": "code",
        "outputId": "fdce79e1-9718-4c99-e4aa-9140bd8bbcfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd Desktop\\Deep Learning\\RL_Dynamic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\chetanm\\Desktop\\Deep Learning\\RL_Dynamic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "855cb7HFTifr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "M_NO                = 1\n",
        "MACHINE             = 'Acer'\n",
        "PATH \t\t\t\t        = 'C:\\\\Users\\\\chetanm\\\\Desktop\\\\Deep Learning\\\\RL_Dynamic'\n",
        "#PATH                = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\Arbi4'\n",
        "\n",
        "MODEL_NAME          = 'Colab_Data_Eval_QL1N3_BAG_Dy'\n",
        "TRAIN_NAME \t\t\t    = 'Colab_Data_Train_QL1N3_BAG_Dy'\n",
        "\n",
        "max_steps           = 75\n",
        "env_dim             = [21, 35]\n",
        "action_dim          = 8\n",
        "state_size          = (env_dim[0] + 1) * env_dim[1]\n",
        "max_iron            = 190\n",
        "\n",
        "lr                  = 3e-4\n",
        "batch_size          = 2048\n",
        "miniBatchSize       = 1024\n",
        "penalty             = -2\n",
        "\n",
        "gamma               = 0.95                             # Discounting rate\n",
        "playTime            = 10\n",
        "\n",
        "warmUpSteps         = 1000 #200\n",
        "experienceLength    = 4000\n",
        "hidden_size         = [512, 256]\n",
        "variance            = 0.4\n",
        "num_episodes        = 1500\n",
        "\n",
        "max_eps             = 1.0\n",
        "min_eps             = 0.01\n",
        "decay_rate          = 0.001\n",
        " \n",
        "tau                 = 0.001\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt7YRpNmTqah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import femm\n",
        "import imageio\n",
        "import warnings\n",
        "from  skimage import img_as_float, img_as_uint\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "import constant_1N3 as con\n",
        "from shutil import copyfile\n",
        "import pickle\n",
        "\n",
        "FEMM_PATH = con.PATH + '\\\\'\n",
        "#FEMM_PATH = 'C:\\\\Users\\\\Arbi\\\\Desktop\\\\Arbi\\\\'\n",
        "MODEL_PATH = con.MODEL_NAME + '\\\\'\n",
        "TRAIN_PATH = con.TRAIN_NAME + '\\\\'\n",
        "\n",
        "class WormFemmEnv3:\n",
        "\n",
        "    def __init__(self, env_dim = [23, 36], startR=3 , startC= 4, max_steps= con.max_steps):\n",
        "        \n",
        "        self.env_dim = env_dim\n",
        "        self.state_hist = []\n",
        "        self.step_count = 0\n",
        "        self.posR = startR\n",
        "        self.posC = startC\n",
        "        self.done = 0.0\n",
        "        self.repeat = 0\n",
        "        self.issue = 0\n",
        "        self.w = 1\n",
        "        self.max_steps = max_steps\n",
        "        self.past_rewards = []\n",
        "        self.past_states = []\n",
        "        self.past_BnStates = []        \n",
        "        self.past_actions = []   \n",
        "        self.net_force = 0.0\n",
        "        self.R = 0.0\n",
        "        self.count = 0\n",
        "        \n",
        "        self.coil_start = 2\n",
        "        self.coil_width = 3\n",
        "        self.coil_length = 9\n",
        "        \n",
        "        self.coil_current_choice = np.array([1, 1.5])\n",
        "        self.coil_turns_choice = np.array([500])\n",
        "        \n",
        "        self.arm_start = 27\n",
        "        self.arm_width = 6\n",
        "        self.arm_length = 15\n",
        "        \n",
        "        self.cont_start_col = self.coil_start + self.coil_width\n",
        "        self.window_width = self.arm_start - (self.coil_start + self.coil_width)\n",
        "        self.window_length = np.max([self.arm_length, 15])\n",
        "        self.frame_idx = 0\n",
        "        self.worm_step_size = 1\n",
        "        self.exp_count = 0\n",
        "\n",
        "        \n",
        "    def create_geo(self):\n",
        "        \n",
        "        femm.openfemm(1);\n",
        "        femm.opendocument(FEMM_PATH + 'actuator.fem');\n",
        "\n",
        "        femm.mi_getmaterial('Air')\n",
        "#        femm.mi_addmaterial('LinearIron', 2100, 2100, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
        "        femm.mi_getmaterial('Cold rolled low carbon strip steel')            \n",
        "        femm.mi_getmaterial('Copper')\n",
        "        femm.mi_addcircprop('icoil', self.coil_current, 1);\n",
        "        \n",
        "        # femm.mi_setblockprop('Air', 0, 10, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        for i in range(17):\n",
        "            for j in range(35):\n",
        "                \n",
        "                femm.mi_selectlabel(i + 0.5, j + 0.5);\n",
        "                \n",
        "        femm.mi_setblockprop('Air', 0, 0.5, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        coil2_start = np.copy(self.coil2_start)\n",
        "        femm.mi_clearselected();\n",
        "        \n",
        "        femm.mi_selectrectangle(0 + 0.1, self.coil_start + 0.1, self.coil_length - 0.1, self.coil_start + self.coil_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()\n",
        "        femm.mi_drawrectangle(0, self.coil_start, self.coil_length, self.coil_start + self.coil_width)\n",
        "        femm.mi_addblocklabel((0 + self.coil_length)/ 2, (2*self.coil_start + self.coil_width)/ 2)\n",
        "        \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectrectangle(0 + 0.1, coil2_start + 0.1, self.coil_length - 0.1, coil2_start + self.coil_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()        \n",
        "        femm.mi_drawrectangle(0, coil2_start, self.coil_length, coil2_start + self.coil_width)\n",
        "        femm.mi_addblocklabel((0 + self.coil_length)/ 2, (2*coil2_start + self.coil_width)/ 2)\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectlabel((0 + self.coil_length)/ 2, (2*coil2_start + self.coil_width)/ 2);\n",
        "        femm.mi_setblockprop('Copper', 0, 0, 'icoil', 0, 1, self.coil_turns); \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectlabel((0 + self.coil_length)/ 2, (2*self.coil_start + self.coil_width)/ 2)\n",
        "        femm.mi_setblockprop('Copper', 0, 0, 'icoil', 0, 1, -self.coil_turns); \n",
        "        \n",
        "        #        femm.mi_setblockprop('Copper', 0, 0.25, 'icoil', 0, 1, 500);        \n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_selectrectangle(0 + 0.1, self.arm_start + 0.1, self.arm_length - 0.1, self.arm_start + self.arm_width - 0.1, 4)\n",
        "        femm.mi_deleteselectedlabels()\n",
        "        femm.mi_deleteselectednodes()\n",
        "        femm.mi_deleteselectedsegments()        \n",
        "        femm.mi_drawrectangle(0, self.arm_start, self.arm_length, self.arm_start + self.arm_width)\n",
        "        femm.mi_addblocklabel((0 + self.arm_length)/ 2, (2*self.arm_start + self.arm_width)/ 2)\n",
        "        femm.mi_selectlabel((0 + self.arm_length)/ 2, (2*self.arm_start + self.arm_width)/ 2);\n",
        "        femm.mi_setblockprop('Cold rolled low carbon strip steel', 0, 0.5, '<None>', 0, 5, 0);\n",
        "        femm.mi_clearselected();\n",
        "                \n",
        "        femm.mi_drawrectangle(0, 100, 100, -100)\n",
        "        femm.mi_addblocklabel(50, 0);\n",
        "        femm.mi_selectlabel(50, 0);\n",
        "        femm.mi_setblockprop('Air', 50, 0, '<None>', 0, 0, 0);\n",
        "        femm.mi_clearselected();\n",
        "\n",
        "        femm.mi_addboundprop('neumann', 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0)\n",
        "        femm.mi_selectrectangle(-0.5, 100.5, 0.2, -100.5, 4)\n",
        "        femm.mi_setsegmentprop('neumann', 0, 0, 0, 5)\n",
        "        femm.mi_clearselected();       \n",
        "        \n",
        "        femm.mi_selectsegment(95, -95)\n",
        "        femm.mi_selectsegment(55, 95)\n",
        "        femm.mi_selectsegment(55, -95)\n",
        "        femm.mi_addboundprop('dirichlet', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
        "        femm.mi_setsegmentprop('dirichlet', 0, 0, 0, 0)\n",
        "        femm.mi_clearselected();       \n",
        "\n",
        "        femm.mi_zoomnatural();\n",
        "        femm.mi_saveas(FEMM_PATH + 'actuator2.fem');  \n",
        "        femm.closefemm()\n",
        "        \n",
        "        #####################################################################################\n",
        "        #                                   VARIABLES                                       #       \n",
        "        #####################################################################################\n",
        "        \n",
        "        # step_count           :       step# in an episode.\n",
        "        # done          :       episode over or not.\n",
        "        # snap          :       store material dist as png (mat2gray).\n",
        "        # act_py        :       action (material dist) sent to matlab.\n",
        "        # state_abs     :       action with discrete values(0/1) & flattened.\n",
        "        # changes       :       changes made between (n-1) & 'n'th actions.\n",
        "        # act_mat       :       action as matlab double.\n",
        "        # count         :       No. of iron (1's) in the action.\n",
        "        # frame_idx     :       episode #.\n",
        "        \n",
        "        #####################################################################################\n",
        "        # mat_res       :       output from matlab:                                         #\n",
        "        #                               nargout= 2 (means 2 variables from matlab)          #\n",
        "        # field_xy      :                   var1 = Bx, By (concatenated) dim: (2, 21, 15)   #\n",
        "        # Fx            :                   var2 = Force magnitude dim: 1(scalar)           #\n",
        "        #####################################################################################\n",
        "        \n",
        "        # reward        :       Reward for the agent (Fx(n) - Fx(n-1)).\n",
        "        # state3        :       environment state for the agent. dim: (3, 21, 15).\n",
        "        \n",
        "        #####################################################################################\n",
        "        \n",
        "    def move(self, mat_state):\n",
        "        if (self.worm_step_size == 3):\n",
        "            mat_state[self.posR -1 : self.posR + 2, self.posC -1 : self.posC + 2] = 1    \n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "        elif (self.worm_step_size == 1 and self.past_actions[-1]<4):\n",
        "            mat_state[self.posR -1 : self.posR + 2, self.posC -1 : self.posC + 2] = 1    \n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "        else:\n",
        "            mat_state[self.posR , self.posC] = 5\n",
        "\n",
        "        return(mat_state)\n",
        "\n",
        "    def reward(self, mat_state):\n",
        "        \n",
        "        #print('entered REWARD process with a shape of:', mat_state.shape)\n",
        "\n",
        "        femm.openfemm(1);\n",
        "        femm.opendocument(FEMM_PATH + 'actuator2.fem');\n",
        "        #femm.mi_addmaterial('LinearIron', 2100, 2100, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
        "\n",
        "        indices = np.nonzero(mat_state[self.buffer:-1, :] - self.state_hist[-1])\n",
        "        \n",
        "        len_ind = np.size(indices[0])\n",
        "        for i in range(len_ind):\n",
        "            #print(indices[0][i], ',', indices[1][i])\n",
        "            femm.mi_selectlabel(indices[0][i] + 0.4, indices[1][i] + 0.4);\n",
        "            femm.mi_deleteselectedlabels()\n",
        "            femm.mi_addblocklabel(indices[0][i] + 0.4, indices[1][i] + 0.4)\n",
        "            femm.mi_clearselected();\n",
        "\n",
        "        for i in range(len_ind):\n",
        "            femm.mi_selectlabel(indices[0][i] + 0.4, indices[1][i] + 0.4);\n",
        "            femm.mi_setblockprop('Cold rolled low carbon strip steel', 0, 0, '<None>', 0, 3, 0);\n",
        "            \n",
        "        femm.mi_clearselected();\n",
        "        femm.mi_zoomnatural()\n",
        "        femm.mi_saveas('actuator2.fem')\n",
        "        femm.mi_analyse(0)\n",
        "        femm.mi_loadsolution()\n",
        "                \n",
        "        Bx, By = femm.mo_getb(0.2, 27.12)\n",
        "        B = np.sqrt(Bx ** 2 + By ** 2)\n",
        "        femm.mo_groupselectblock(5)\n",
        "        self.net_force = femm.mo_blockintegral(19) *(-1)\n",
        "                  \n",
        "        Bxy_state = np.zeros([self.env_dim[0]-self.buffer, self.env_dim[1], 2])\n",
        "        for r in range(Bxy_state.shape[0]):\n",
        "            for c in range(self.env_dim[1]):\n",
        "                Bxy_state[r, c, :] = femm.mo_getb(r+0.5, c+0.5)\n",
        "            \n",
        "        self.B_state = np.sum(np.square(Bxy_state), axis=-1)*1000        \n",
        "                   \n",
        "        femm.closefemm()\n",
        "        \n",
        "        reward = B*1000\n",
        "                \n",
        "        return(reward)\n",
        "        \n",
        "    def clean_up(self, mat_state):\n",
        "        \n",
        "        mat_state[self.buffer: self.buffer + self.coil_length, \n",
        "                  self.coil_start: self.coil_start + self.coil_width] = self.coil_turns * self.coil_current\n",
        "        coil2_start = np.copy(self.coil2_start)\n",
        "        mat_state[:, 0:self.coil_start] = 3\n",
        "        mat_state[self.buffer: self.buffer + self.coil_length, \n",
        "                  coil2_start : coil2_start + self.coil_width] = self.coil_turns * self.coil_current\n",
        "        \n",
        "        mat_state[0:self.buffer, :] = 3      \n",
        "        mat_state[:, self.arm_start- 1] = 3\n",
        "        mat_state[self.window_length + self.buffer:, :] = 3\n",
        "        mat_state[self.buffer: self.buffer + self.arm_length, \n",
        "                  self.arm_start : self.arm_start + self.arm_width] = 4\n",
        "        \n",
        "        mat_state[mat_state == 5] = 1\n",
        "        mat_state[self.posR, self.posC] = 5\n",
        "\n",
        "        return mat_state\n",
        "    \n",
        "    ##### ACTIONS #######\n",
        "    # 0     : RIGHT     #\n",
        "    # 1     : LEFT      #\n",
        "    # 2     : UP        #\n",
        "    # 3     : DOWN      #\n",
        "    ##-----------------##\n",
        "    # 4     : RIGHT     #\n",
        "    # 5     : LEFT      #\n",
        "    # 6     : UP        #\n",
        "    # 7     : DOWN      #\n",
        "    #####################\n",
        "    \n",
        "    def step(self, action):\n",
        "        \n",
        "        state = self.past_states[-1]\n",
        "        self.step_count +=1\n",
        "        self.R = 0.0\n",
        "        self.count = 0\n",
        "\n",
        "        if action == 0:\n",
        "            self.worm_step_size = 1\n",
        "            self.posC += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.issue = 2\n",
        "                self.posC -= self.worm_step_size\n",
        "                        \n",
        "        elif action == 1:\n",
        "            self.worm_step_size = 1                               \n",
        "            self.posC -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posC += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 2:\n",
        "            self.worm_step_size = 1                               \n",
        "            self.posR -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posR += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 3:\n",
        "            self.worm_step_size = 1    \n",
        "            self.posR += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 1\n",
        "            else:\n",
        "                self.posR -= self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 4:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3  \n",
        "                \n",
        "            self.posC += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.issue = 2\n",
        "                self.posC -= self.worm_step_size\n",
        "                        \n",
        "        elif action == 5:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3                    \n",
        "            \n",
        "            self.posC -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posC += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 6:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3\n",
        "                \n",
        "            self.posR -= self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                self.issue = 1\n",
        "                state2 = self.move(state)\n",
        "            else:\n",
        "                self.posR += self.worm_step_size\n",
        "                self.issue = 2\n",
        "        \n",
        "        elif action == 7:\n",
        "            if self.past_actions[-1]<4:\n",
        "                self.worm_step_size = 1\n",
        "            else:\n",
        "                self.worm_step_size = 3\n",
        "            self.posR += self.worm_step_size\n",
        "            \n",
        "            if state[self.posR, self.posC] == 0:    \n",
        "                state2 = self.move(state)\n",
        "                self.issue = 0\n",
        "            elif state[self.posR, self.posC] == 1:\n",
        "                state2 = self.move(state)\n",
        "                self.issue = 1\n",
        "            else:\n",
        "                self.posR -= self.worm_step_size\n",
        "                self.issue = 2\n",
        "                \n",
        "        if self.issue == 0:    \n",
        "            #print('Going for reward with a shape of:', state2.shape)\n",
        "            new_state = self.clean_up(state2)\n",
        "            try:\n",
        "                F = self.reward(new_state)\n",
        "            except:\n",
        "                try:\n",
        "                    F = self.reward(new_state)\n",
        "                except:\n",
        "                    F = self.reward(new_state)\n",
        "                    \n",
        "            self.R = F - self.past_rewards[-1]\n",
        "#            R = F\n",
        "            self.past_rewards.append(F)\n",
        "           \n",
        "        elif self.issue == 1:\n",
        "                                 \n",
        "#            self.R = con.penalty + 2\n",
        "#            new_state = self.clean_up(state2)\n",
        "#            self.past_rewards.append(F)\n",
        "            new_state = self.clean_up(state2)\n",
        "            try:\n",
        "                F = self.reward(new_state)\n",
        "            except:\n",
        "                try:\n",
        "                    F = self.reward(new_state)\n",
        "                except:\n",
        "                    F = self.reward(new_state)\n",
        "            \n",
        "            self.R = F - self.past_rewards[-1]\n",
        "#            R = F\n",
        "            self.past_rewards.append(F)\n",
        "           \n",
        "        elif self.issue == 2:\n",
        "            \n",
        "            new_state = state\n",
        "#            self.R = constants_DQL.penalty\n",
        "#            F = self.past_rewards[-1]\n",
        "#            self.past_rewards.append(F)\n",
        "            \n",
        "            #print('NOT Going for REWARD with a shape of:', new_state.shape)\n",
        "            #print('NOT Going for REWARD coz of state[',self.posR,',',self.posC, '] = ', state[self.posR, self.posC])\n",
        "            \n",
        "            if state[self.posR, self.posC] == 2:\n",
        "                self.R = con.penalty\n",
        "                # coil\n",
        "            elif state[self.posR, self.posC] == 3:\n",
        "                self.R = con.penalty - 1\n",
        "                # outside air\n",
        "            else:\n",
        "                self.R = con.penalty\n",
        "                # Repeat\n",
        "                \n",
        "        unique, counts = np.unique(new_state, return_counts=True)\n",
        "        index_1 = np.where (unique == 1)\n",
        "        index_5 = np.where (unique == 5)\n",
        "        try:\n",
        "            self.count = np.asscalar(counts[index_1[0]] + 1)\n",
        "            \n",
        "        except:\n",
        "            self.count = 0        \n",
        "        \n",
        "        if self.count > con.max_iron or self.step_count == self.max_steps:\n",
        "            self.done = 1.0\n",
        "            #print(\"Steps:\", self.step_count, \"Iron Count:\", count, \"Reward\", self.past_rewards[-1])\n",
        "#            R = self.reward(new_state)\n",
        "#            print(\"Steps:\", self.step_count, \"Iron Count:\", count, \"Reward\", R)    \n",
        "#        elif count > 80:\n",
        "#            R += 1\n",
        "#        elif count > 80 and self.issue == 2:\n",
        "#            R = 0\n",
        "#        print(\"Steps:\", self.step_count, \"Iron Count:\", count, \"Reward\", R, \"Action- \", action)\n",
        "        \n",
        "        if new_state[self.posR, self.posC] != 5:\n",
        "            raise('Problem in code. Worm outside the geometry')\n",
        "            \n",
        "        elif np.asscalar(counts[index_5[0]]) > 1:\n",
        "            raise('Problem in code. More than one worm heads <<MEDUSA>>')\n",
        "        \n",
        "        self.past_states.append(np.copy(new_state))\n",
        "        self.past_actions.append(np.copy(action))\n",
        "#        , \"Changes:\", changes-22\n",
        "        \n",
        "        new_state2 = new_state[self.buffer:, :]\n",
        "        new_state2[:, self.arm_start-1] = self.B_state[:, self.arm_start-1]\n",
        "        self.past_BnStates.append(np.copy(new_state2))                \n",
        "        \n",
        "        new_state3 = np.expand_dims(new_state2, 0)\n",
        "\t\t\n",
        "#        fd = open(eps_dir + str(frame_idx) + '.csv','a')\n",
        "#        writer=csv.writer(fd,delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
        "#        writer.writerow([self.step_count, R, count])\n",
        "#        fd.close()\n",
        "#        new_state = np.expand_dims(new_state, axis= -1)    \n",
        "#        state_index = np.argmax(new_state[1:16, 5:26])\n",
        "        new_state4 = np.copy(new_state3)\n",
        "        return(new_state4, self.R, self.done, {})\n",
        "        \n",
        "          \n",
        "    def reset(self, geo_dynamic= False):\n",
        "        \n",
        "    #    state_dim = [21, 31]  # size of geometry\n",
        "\n",
        "        self.worm_step_size = 3            \n",
        "        self.past_rewards = []\n",
        "        self.past_states = []\n",
        "        self.past_actions = []\n",
        "        self.state_hist = []        \n",
        "        self.step_count = 0\n",
        "        self.posC = 1+self.w\n",
        "        self.posR = 1+self.w\n",
        "        self.done = 0.0\n",
        "        self.max_steps = con.max_steps\n",
        "        self.past_rewards = []\n",
        "        self.past_states = []\n",
        "        self.net_force = 0.0\n",
        "        self.R = 0.0\n",
        "        self.count = 0         \n",
        "        self.exp_count = 0      \n",
        "\n",
        "        if geo_dynamic == True:\n",
        "            self.coil_start = np.random.randint(1, 6)        \n",
        "            self.arm_start = np.random.randint(25, 31)\n",
        "            self.coil_width = np.random.randint(3, 8- self.coil_start + 1)\n",
        "            self.coil2_start = np.random.randint(self.coil_start + self.coil_width + 4, self.coil_start + self.coil_width + 8)  \n",
        "            self.arm_width = np. random.randint(3, 35 - self.arm_start)\n",
        "            self.arm_length = np.random.randint(13, 17)\n",
        "            self.window_length = np.random.randint(self.arm_length, 17)\n",
        "            self.coil_length = np.random.randint(6, self.window_length -5)\n",
        "            self.window_width = self.arm_start - self.coil_start - self.coil_width - 1\n",
        "            self.coil_turns = 500\n",
        "            self.coil_current = 1\n",
        "\n",
        "        else:               \n",
        "            self.coil_start = 2\n",
        "            self.arm_start = 27\n",
        "            self.coil_width = 3\n",
        "            self.coil2_start = 8 + 8 - (self.coil_start + self.coil_width)\n",
        "            self.arm_width = 6\n",
        "            self.arm_length = 15\n",
        "            self.window_length = 15\n",
        "            self.coil_length = 9\n",
        "            self.window_width = 21\n",
        "            self.coil_turns = np.random.choice(self.coil_turns_choice)\n",
        "            self.coil_current = np.random.choice(self.coil_current_choice)\n",
        "\n",
        "#        self.create_geo()\n",
        "                \n",
        "        #print('RESETing with a state[',self.posR,',',self.posC, '] = ', ob[self.posR, self.posC])        \n",
        "        \n",
        "        self.create_geo()        \n",
        "        self.buffer = 3\n",
        "        ob = np.ones(self.env_dim)*3\n",
        "\n",
        "        ob[self.buffer : self.buffer + self.window_length, \n",
        "           self.coil_start + self.coil_width : \n",
        "               self.coil_start + self.coil_width + self.window_width] = 0\n",
        "\n",
        "        ob[self.buffer: self.buffer + self.coil_length, \n",
        "           self.coil_start: self.coil_start + self.coil_width] = self.coil_turns * self.coil_current\n",
        "           \n",
        "        ob[self.buffer: self.buffer + self.coil_length , \n",
        "           self.coil2_start : self.coil2_start + self.coil_width] = self.coil_turns * self.coil_current\n",
        "\n",
        "        ob[self.buffer: self.buffer + self.arm_length, \n",
        "           self.arm_start : self.arm_start + self.arm_width] = 4                       \n",
        "           \n",
        "#        ob = np.ones(self.env_dim)*3\n",
        "#        ob[1:self.window_length + 1, self.coil_start + self.coil_width : \n",
        "#            self.coil_start + self.coil_width + self.window_width] = 0\n",
        "#        ob[1:self.coil_length + 1, self.coil_start: self.coil_start + self.coil_width] = 2\n",
        "#        \n",
        "#        ob[1:self.coil_length + 1, self.coil2_start : self.coil2_start + self.coil_width] = 2\n",
        "#        ob[1:self.arm_length + 1, self.arm_start : self.arm_start + self.arm_width] = 4\n",
        "        \n",
        "        #print('RESETing with a state[',self.posR,',',self.posC, '] = ', ob[self.posR, self.posC])\n",
        "        \n",
        "        self.posC = self.coil_start + self.coil_width + self.worm_step_size -2\n",
        "        self.posR = self.worm_step_size + 1        \n",
        "                  \n",
        "        self.state_hist.append((np.copy(ob[self.buffer:-1, :])))        \n",
        "\n",
        "        ob = self.move(ob)    \n",
        "#        ob = self.clean_up(ob)\n",
        "        self.past_states.append(np.copy(ob))\n",
        "        self.past_actions.append(4)\n",
        "        F = self.reward(ob)\n",
        "        self.past_rewards.append(np.copy(F))\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        ob_image = np.copy(ob)\n",
        "        ob_image[ob==self.coil_turns * self.coil_current] = 2\n",
        "        imageio.imwrite(FEMM_PATH + 'first.png', img_as_uint(ob_image/5))\n",
        "        self.frame_idx += 1\n",
        "      #  print('reset done')\n",
        "#        state_index = np.argmax(ob[1:16, 5:26])        \n",
        "        \n",
        "        ob2 = ob[self.buffer:, :]\n",
        "        ob2[:, self.arm_start-1] = self.B_state[:, self.arm_start-1]\n",
        "        \n",
        "#        coil_indices = np.where(ob2 == 2)\n",
        "#        self.AN_state = np.zeros_like(ob2)\n",
        "#        self.AN_state[coil_indices] = self.coil_turns * self.coil_current\n",
        "        \n",
        "        #ob3 = np.dstack((ob2, np.copy(self.B_state)))        \n",
        "        self.past_BnStates.append(np.copy(ob2))                \n",
        "        ob3 = np.expand_dims(ob2, 0)\n",
        "\n",
        "        ob4 = np.copy(ob3)\n",
        "                      \n",
        "        return(ob4)\n",
        "        \n",
        "    def render(self, train= False):\n",
        "\n",
        "        if train == True:    \n",
        "            eps_dir = FEMM_PATH + TRAIN_PATH + '\\\\Eps_' + str(self.frame_idx) + '\\\\'\n",
        "        else:\n",
        "            eps_dir = FEMM_PATH + MODEL_PATH + 'Eps_' + str(self.frame_idx) + '\\\\'   \n",
        "        \n",
        "        if not (os.path.exists(FEMM_PATH + MODEL_PATH)):\n",
        "            os.mkdir(FEMM_PATH + MODEL_PATH)\n",
        "\n",
        "        if not (os.path.exists(eps_dir)):\n",
        "            os.mkdir(eps_dir)\n",
        "\n",
        "        toP_image = np.copy(self.past_states[-1])\n",
        "        toP_image[toP_image==self.coil_turns * self.coil_current] = 2                    \n",
        "\n",
        "        toP_image = toP_image[self.buffer:-1, :]\n",
        "#        toP[toP == 1] = 5\n",
        "        toP_image = toP_image/np.max(toP_image)\n",
        "        imageio.imwrite(eps_dir + str(self.step_count) + '_Issue-' + str(self.issue) + '_Action-' + str(self.past_actions[-1]) + '_reward-' + str(self.R) + '.png', img_as_uint(toP_image))\n",
        "        \n",
        "def save_file(src, fileName):\n",
        "    dst = FEMM_PATH + MODEL_PATH + fileName + '.py'    \n",
        "    copyfile(src, dst)\n",
        "\n",
        "def get_file(path_file):\n",
        "    path_components = path_file.split('\\\\')\n",
        "    for component in path_components:\n",
        "        file_sre = re.search('(.*).py', component)\n",
        "#        file_name = file_sre.group(1)\n",
        "    return file_sre     \n",
        "\n",
        "def get_file_back_slash(path_file):\n",
        "    path_components = path_file.split('/')\n",
        "    for component in path_components:\n",
        "        file_sre = re.search('(.*).py', component)\n",
        "#        file_name = file_sre.group(1)\n",
        "    return file_sre     \n",
        "\n",
        "def create_txt_file(txt):\n",
        "    with open(FEMM_PATH + MODEL_PATH + \"history.txt\", \"w\") as myfile:\n",
        "        myfile.write(txt)\n",
        "        \n",
        "def append_txt_file(txt):\n",
        "    with open(FEMM_PATH + MODEL_PATH + \"history.txt\", \"a\") as myfile:\n",
        "            myfile.write(txt)\n",
        "      \n",
        "def save_obj(obj, name):    \n",
        "    with open(FEMM_PATH + MODEL_PATH + name + '.pkl', 'wb') as f:        \n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)            \n",
        "\n",
        "def load_obj(name ):    \n",
        "    with open('obj/' + name + '.pkl', 'rb') as f:        \n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrx6qsw0Tshg",
        "colab_type": "code",
        "outputId": "4c56dd7a-9bf8-4b60-ef57-0f90680e7c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from env_Worm_Dy_NA_stacked import WormFemmEnv3\n",
        "import constant_1N3 as con\n",
        "\n",
        "env = WormFemmEnv3(max_steps=con.max_steps)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "    \n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=[1,1])\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        #self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=[1,1])\n",
        "        #self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=[1,1], stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "#        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
        "#            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "#        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "#        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = int(h/2 * w/2 * 32)\n",
        "#        print(h, w, linear_input_size, outputs)\n",
        "        self.head1 = nn.Linear(linear_input_size, int(linear_input_size/2))                \n",
        "        self.head2 = nn.Linear(int(linear_input_size/2), int(linear_input_size/4))\n",
        "        self.tail = nn.Linear(int(linear_input_size/4), outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "#        print(x.shape)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        #x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "#        print(x.shape)\n",
        "        x = F.relu(self.head1(x.view(x.size(0), -1)))\n",
        "        x = F.relu(self.head2(x))\n",
        "        return self.tail(x)\n",
        "    \n",
        "BATCH_SIZE = 512\n",
        "GAMMA = 0.95\n",
        "EPS_START = 0.95\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 25\n",
        "TARGET_UPDATE = 20\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "#init_screen = get_screen()\n",
        "screen_height, screen_width = env.env_dim\n",
        "print(screen_height, screen_width)\n",
        "# Get number of actions from gym action space\n",
        "n_actions = con.action_dim\n",
        "\n",
        "policy_net = DQN(screen_height-3, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height-3, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(5000)\n",
        "good_memory = ReplayMemory(5000)\n",
        "\n",
        "def select_action(state):\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * env.frame_idx / EPS_DECAY)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state.float()).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        env.exp_count += 1\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "#state = env.reset()\n",
        "#state = torch.tensor(state, device=device)\n",
        "#state = torch.unsqueeze(state, 0)        \n",
        "#action = select_action(state)        \n",
        "#next_state, reward, done, _ = env.step(action.item())                   \n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE and len(good_memory) < (3*env.max_steps):\n",
        "        loss = 0.0\n",
        "        return loss\n",
        "    \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    good_transitions = good_memory.sample(int(2*env.max_steps))\n",
        "    new_transitions = transitions + good_transitions\n",
        "    mix_transitions = random.sample(new_transitions, BATCH_SIZE)\n",
        "    \n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*mix_transitions))\n",
        "    \n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
        "    \n",
        "    # Issue 'non_final_next_states' is torch.float32\n",
        "    non_final_next_states = torch.cat([torch.unsqueeze(s, 0) for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "             \n",
        "#    state_batch = torch.cat(batch.state)\n",
        "    state_batch = torch.cat([torch.unsqueeze(s, 0) for s in batch.state])\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)                              \n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch.float()).gather(1, action_batch.view(-1,1).long())\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states.float()).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "num_episodes = 500\n",
        "episode_net_force = []\n",
        "play_episode_net_force = []\n",
        "i_episode = env.frame_idx\n",
        "good_mem_avg = 0.0\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    reset_frame = env.reset()\n",
        "    reset_frame = torch.tensor(reset_frame, device=device, dtype=torch.float64)\n",
        "    state = torch.cat([reset_frame, reset_frame, reset_frame, reset_frame], dim=0)                      \n",
        "              \n",
        "    i_episode = env.frame_idx\n",
        "    eps_loss = 0\n",
        "    eps_memory = []\n",
        "        \n",
        "#    last_screen = get_screen()\n",
        "#    current_screen = get_screen()\n",
        "#    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "#        print(state.shape)\n",
        "        action = select_action(torch.unsqueeze(state, 0))\n",
        "        next_frame, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        \n",
        "        # Observe new state\n",
        "#        last_screen = current_screen\n",
        "#        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_frame = torch.tensor(next_frame, device=device, dtype=torch.float64)\n",
        "            next_state = torch.cat([next_frame, state[0:-1, :,:]], dim=0)      \n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "#        print(state.shape)\n",
        "        \n",
        "#        print(next_state.shape)\n",
        "        memory.push(state, action, next_state, reward.float())\n",
        "#        print(next_state.shape)\n",
        "        eps_memory.append(Transition(state, action, next_state, reward.float()))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        #env.render(train=True)\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        loss = optimize_model()\n",
        "        eps_loss += loss\n",
        "        if done:\n",
        "            print('Episode {:d}, Exp {:d}, Amp-N {:3.2f}, loss= {:2.3f} Net Force= {:2.5f}'\n",
        "                  .format(env.frame_idx, env.exp_count, env.coil_turns*env.coil_current, eps_loss/env.step_count, env.net_force))\n",
        "            episode_net_force.append(env.net_force)\n",
        "            \n",
        "            if (sum(episode_net_force)/len(episode_net_force)) <= env.net_force:\n",
        "                for i in range(len(eps_memory)):\n",
        "                    state, action, next_state, reward = eps_memory[i]\n",
        "                    good_memory.push(state, action, next_state, reward)\n",
        "                    \n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        \n",
        "        reset_frame = env.reset()\n",
        "        reset_frame = torch.tensor(reset_frame, device=device, dtype=torch.float64)           \n",
        "        state = torch.cat([reset_frame, reset_frame, reset_frame, reset_frame], dim=0)                          \n",
        "    \n",
        "        for t in count():\n",
        "        # Select and perform an action\n",
        "#        print(state.shape)\n",
        "            action = target_net(torch.unsqueeze(state, 0).float()).max(1)[1].view(1, 1)\n",
        "            next_frame, reward, done, _ = env.step(action.item())\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "        \n",
        "        # Observe new state\n",
        "#        last_screen = current_screen\n",
        "#        current_screen = get_screen()\n",
        "            if not done:                \n",
        "                next_frame = torch.tensor(next_frame, device=device, dtype=torch.float64)\n",
        "                next_state = torch.cat([next_frame, state[0:-1, :,:]], dim=0)                               \n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "        # Move to the next state\n",
        "            state = next_state\n",
        "            env.render(train=False)\n",
        "            if done:\n",
        "                print('\\n \\t Episode {:d}, Exp {:d}, Amp-N {:3.2f}, Net Force= {:2.5f}'\n",
        "                      .format(env.frame_idx, env.exp_count, env.coil_turns*env.coil_current, env.net_force))\n",
        "                play_episode_net_force.append(env.net_force)\n",
        "                break\n",
        "            \n",
        "print('Complete')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 501, Net Force= 24.91337\n",
            "Episode 502, Net Force= 0.00332\n",
            "Episode 503, Net Force= 0.05711\n",
            "Episode 504, Net Force= 0.12188\n",
            "Episode 505, Net Force= 21.92158\n",
            "Episode 506, Net Force= 0.05147\n",
            "Episode 507, Net Force= 0.41154\n",
            "Episode 508, Net Force= 0.00972\n",
            "Episode 509, Net Force= 0.01089\n",
            "Episode 510, Net Force= 26.09894\n",
            "Episode 511, Net Force= 0.01149\n",
            "Episode 512, Net Force= 0.01738\n",
            "Episode 513, Net Force= 0.00963\n",
            "Episode 514, Net Force= 14.52533\n",
            "Episode 515, Net Force= 0.03156\n",
            "Episode 516, Net Force= 4.09244\n",
            "Episode 517, Net Force= 0.16976\n",
            "Episode 518, Net Force= 0.01262\n",
            "Episode 519, Net Force= 1.90709\n",
            "Episode 520, Net Force= 0.01386\n",
            "Episode 521, Net Force= 0.08364\n",
            "Episode 522, Net Force= 0.22431\n",
            "Episode 523, Net Force= 4.89251\n",
            "Episode 524, Net Force= 10.24755\n",
            "Episode 525, Net Force= 19.67229\n",
            "Episode 526, Net Force= 0.18723\n",
            "Episode 527, Net Force= 0.02793\n",
            "Episode 528, Net Force= 0.00302\n",
            "Episode 529, Net Force= 0.03488\n",
            "Episode 530, Net Force= 7.22705\n",
            "Episode 531, Net Force= 0.00425\n",
            "Episode 532, Net Force= 9.20635\n",
            "Episode 533, Net Force= 0.03055\n",
            "Episode 534, Net Force= 0.02451\n",
            "Episode 535, Net Force= 0.01935\n",
            "Episode 536, Net Force= 29.93294\n",
            "Episode 537, Net Force= 0.05499\n",
            "Episode 538, Net Force= 0.04721\n",
            "Episode 539, Net Force= 0.58895\n",
            "Episode 540, Net Force= 0.00768\n",
            "Episode 541, Net Force= 0.07889\n",
            "Episode 542, Net Force= 0.04458\n",
            "Episode 543, Net Force= 0.03568\n",
            "Episode 544, Net Force= 0.10089\n",
            "Episode 545, Net Force= 0.05937\n",
            "Episode 546, Net Force= 0.00757\n",
            "Episode 547, Net Force= 17.32831\n",
            "Episode 548, Net Force= 0.02525\n",
            "Episode 549, Net Force= 24.66898\n",
            "Episode 550, Net Force= 0.12993\n",
            "Episode 551, Net Force= 21.11039\n",
            "Episode 552, Net Force= 0.15780\n",
            "Episode 553, Net Force= 0.07380\n",
            "Episode 554, Net Force= 0.02346\n",
            "Episode 555, Net Force= 21.20517\n",
            "Episode 556, Net Force= 0.19135\n",
            "Episode 557, Net Force= 17.87222\n",
            "Episode 558, Net Force= 25.04157\n",
            "Episode 559, Net Force= 0.00335\n",
            "Episode 560, Net Force= 0.21619\n",
            "Episode 561, Net Force= 1.07659\n",
            "Episode 562, Net Force= 0.28193\n",
            "Episode 563, Net Force= 0.53649\n",
            "Episode 564, Net Force= 0.10256\n",
            "Episode 565, Net Force= 15.82886\n",
            "Episode 566, Net Force= 7.35779\n",
            "Episode 567, Net Force= 0.14372\n",
            "Episode 568, Net Force= 0.58982\n",
            "Episode 569, Net Force= 0.08481\n",
            "Episode 570, Net Force= 1.02724\n",
            "Episode 571, Net Force= 0.02285\n",
            "Episode 572, Net Force= 0.58286\n",
            "Episode 573, Net Force= 0.22398\n",
            "Episode 574, Net Force= 0.61257\n",
            "Episode 575, Net Force= 0.07966\n",
            "Episode 576, Net Force= 23.83150\n",
            "Episode 577, Net Force= 24.31138\n",
            "Episode 578, Net Force= 0.20010\n",
            "Episode 579, Net Force= 0.59525\n",
            "Episode 580, Net Force= 0.71940\n",
            "Episode 581, Net Force= 0.91916\n",
            "Episode 582, Net Force= 0.77276\n",
            "Episode 583, Net Force= 0.04127\n",
            "Episode 584, Net Force= 0.02453\n",
            "Episode 585, Net Force= 2.11804\n",
            "Episode 586, Net Force= 0.36062\n",
            "Episode 587, Net Force= 0.18271\n",
            "Episode 588, Net Force= 0.70458\n",
            "Episode 589, Net Force= 0.56008\n",
            "Episode 590, Net Force= 0.67156\n",
            "Episode 591, Net Force= 0.00118\n",
            "Episode 592, Net Force= 0.09087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-vB_TLCW0Y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}